# -*- coding: utf-8 -*-
"""Media Coverage and Stock Returns

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pLKEN6aHdClHBWlrMrFc4vr5qtzHTvOz
"""

import pandas as pd
import statsmodels.formula.api as smf
import numpy as np
import matplotlib.pyplot as plt

!pip install pandasql
import os
from datetime import datetime
from sklearn.model_selection import train_test_split
from statsmodels.tsa.stattools import adfuller
from statsmodels.tsa.seasonal import seasonal_decompose
from statsmodels.tsa.arima_model import ARIMA
from sklearn.metrics import mean_squared_error, mean_absolute_error

import math
import matplotlib.pyplot as plt
from datetime import datetime
import seaborn as sns
import pandasql as ps

from sqlite3 import connect
conn=connect(':memory:')

from google.colab import drive
drive.mount("/content/gdrive", force_remount=True)

#Import CRSP data
filepath="/content/gdrive/MyDrive/FANG REPLICATION/"
file="CRSPMONTHLY.csv"
crsp=pd.read_csv(filepath + file)
crsp.columns

crsp

crsp['date'] = pd.to_datetime(crsp['date'], format="%Y%m%d")


#parse the relevant variables

crsp_parsed=crsp[['PERMNO', 'date', 'TICKER', 'COMNAM', 'RET', 'SHROUT', 'ALTPRC', 'EXCHCD', 'SHRCD', 'SICCD', 'DLRET', 'DLSTCD']]

crsp=crsp_parsed

#convert variables

'''
VARIABLE DICTIONARY:
PERMNO: security identifier
date: month identifier
RET: return
SHROUT: shares outstanding
ALTPRC: last traded price in a month
EXCHCD: exchange code
SHRCD: share code
SICCD: industry code
DLRET: delisting return
DLSTCD: delisting code
'''

x=['PERMNO', 'RET', 'SHROUT', 'ALTPRC', 'EXCHCD', 'SHRCD', 'SICCD', 'DLRET', 'DLSTCD']

for x in x:
  crsp[x]=pd.to_numeric(crsp[x], errors='coerce')

#convert returns to percent:
crsp['RET']*=100
crsp['DLRET']*=100


#add market cap column
crsp['mkt_cap']=abs(crsp['SHROUT'] * crsp['ALTPRC'])/1000
crsp['mkt_cap'].replace(0, np.NaN, inplace=True)

#rename variables to lower case
crsp= crsp.rename(columns=str.lower)


#filter for relevant exchanges
exchange_mapping = {
    1: 'NYSE', 31: 'NYSE',
    2: 'AMEX', 32: 'AMEX',
    3: 'NASDAQ', 33: 'NASDAQ'
}
#create dictionary to map values of the 'exchcd' column



crsp['exchange'] = np.select(
    [crsp['exchcd'].isin(exchange_mapping.keys()), crsp['exchcd'].notnull()],
    [crsp['exchcd'].map(exchange_mapping), "Other"],
    default='Other')

#only keep NYSE, AMEX and NASDAQ stocks
crsp=crsp[crsp['exchange'] == "NYSE"]

#here we modify the code to avoid looping over the entire dataframe
#we do this by creating masks which are a boolean array of true/false values
#we then use the .loc function to perform vectorised operations
crsp['ret_adj'] = crsp['ret']

mask1 = pd.isnull(crsp['dlstcd'])
crsp.loc[mask1, 'ret_adj'] = crsp.loc[mask1, 'ret']

mask2 = pd.notnull(crsp['dlstcd']) & pd.notnull(crsp['dlret'])
crsp.loc[mask2, 'ret_adj'] = crsp.loc[mask2, 'dlret']

mask3 = ((551 <= crsp['dlstcd']) & (crsp['dlstcd'] <= 574)) | (crsp['dlstcd'].isin([500, 520, 580, 584]))
crsp.loc[mask3, 'ret_adj'] = -30

crsp.loc[~(mask1 | mask2 | mask3), 'ret_adj'] = -100

crsp.drop(['shrcd', 'dlret', 'dlstcd'], inplace=True, axis=1)

crsp = crsp[crsp['date'] > '2016-01-01']

crsp['year']=crsp['date'].dt.year

#exclude financial firms
#crsp=crsp[(crsp['siccd']<6000) |(crsp['siccd']>6999) ]

# Number of unique 'ticker' values in the entire DataFrame
total_unique_tickers = crsp['ticker'].nunique()

# Number of unique 'ticker' values where 'altprc' is less than 5
filtered_unique_tickers = crsp[crsp['altprc'] < 5]['ticker'].nunique()

print(f"Total number of unique tickers: {total_unique_tickers}")
print(f"Number of unique tickers where altprc is less than 5: {filtered_unique_tickers}")

crsp = crsp[crsp['altprc'] >= 5]

main_df=crsp[['year', 'ticker']].drop_duplicates()
main_df
# Step 1: Make a list of unique tickers in crsp where altprc is less than 5
low_priced_tickers = crsp[crsp['altprc'] < 5]['ticker'].unique()

# Step 2: Filter out these tickers from main_df
main_df = main_df[~main_df['ticker'].isin(low_priced_tickers)]

len(main_df['ticker'].unique())

"""# Import News Data"""

file_path = "/content/gdrive/MyDrive/"
news=pd.read_csv(file_path + 'clean-articles.csv')
news

publishers_list = ["The New York Times", "USA Today", "Washington Post", "Wall Street Journal Abstracts"]
news_filtered = news[news['publisher'].isin(publishers_list)]

# Convert 'publishedDate' to datetime
news_filtered['publishedDate'] = pd.to_datetime(news_filtered['publishedDate'])

# Extract year and create a new column
news_filtered['year'] = news_filtered['publishedDate'].dt.year
grouped_df = news_filtered.groupby(['year', 'ticker', 'publisher']).size().reset_index(name='count')
grouped_df

wide_df = grouped_df.pivot(index=['year', 'ticker'], columns='publisher', values='count')
wide_df = wide_df.fillna(0)
news=wide_df
news.reset_index(inplace=True)
news.rename(columns={'The New York Times': 'NYT', 'USA Today': 'USAT', 'Wall Street Journal Abstracts':'WSJ', 'Washington Post': 'WP'}, inplace=True)

merged_df = main_df.merge(news, on=['year', 'ticker'], how='left')
main=merged_df
main.fillna(0)

import numpy as np

main['nyt_dummy'] = np.where(main['NYT'] > 0, 1, 0)
main['usat_dummy'] = np.where(main['USAT'] > 0, 1, 0)
main['wsj_dummy'] = np.where(main['WSJ'] > 0, 1, 0)
main['wp_dummy'] = np.where(main['WP'] > 0, 1, 0)

main['excl_wsj_dummy'] = np.where((main['WP'] > 0)|(main['USAT'] > 0)|(main['NYT'] > 0), 1, 0)

main['coverage_dummy'] = (main['nyt_dummy'] | main['usat_dummy'] | main['wsj_dummy'] | main['wp_dummy']).astype(int)

fraction_nyt_2019 = main[main['year'] == 2021]['wsj_dummy'].mean()
fraction_nyt_2019

main[main['year'] == 2018]['wsj_dummy'].value_counts()

main.fillna(0)
years =main.groupby('year')['count'].sum()
years

"""# Table 1

**Unconditional Coverage Statistics:**
"""

main_df=crsp[['year', 'ticker']].drop_duplicates()
main_df
# Step 1: Make a list of unique tickers in crsp where altprc is less than 5
low_priced_tickers = crsp[crsp['altprc'] < 5]['ticker'].unique()

# Step 2: Filter out these tickers from main_df
main_df = main_df[~main_df['ticker'].isin(low_priced_tickers)]

news['publishedDate'] = pd.to_datetime(news['publishedDate'])

# Extract year and create a new column
news['year'] = news['publishedDate'].dt.year
grouped_df = news.groupby(['year', 'ticker']).size().reset_index(name='count')
merged_df = main_df.merge(grouped_df, on=['year', 'ticker'], how='left')
main=merged_df
main.fillna(0)

len(main['ticker'].unique())

main['coverage_dummy']=np.where(main['count']>0, 1, 0)

t1_uc = pd.DataFrame(columns=['Year', 'All papers'])
years = list(range(2017, 2023))
for year in years:
  t1_uc.loc[len(t1_uc)] = [year, main[main['year']==year]['coverage_dummy'].mean()]

all_df = main.groupby('ticker')['count'].sum().reset_index()
all_df['coverage_dummy'] = np.where(all_df['count'] > 0, 1, 0)
t1_uc.loc[len(t1_uc)]=['All years', all_df['coverage_dummy'].mean()]

t1_uc

"""**Conditional Coverage Statistics**"""

c_news=main[main['coverage_dummy']==1]
t1_c = pd.DataFrame(columns=['Year', 'Mean', 'Median'])
for year in years:
  t1_c.loc[len(t1_c)] = [year, int(main[main['year']==year]['count'].mean()), int(main[main['year']==year]['count'].median()) ]
c_all=all_df[all_df['coverage_dummy']==1]
t1_c.loc[len(t1_c)]=['All years', int(c_all['count'].mean()), int(c_all['count'].median())]

t1_c

"""# Figure 1"""

main.fillna(0)

crsp['siccd']=crsp['siccd'].astype(str).str[0]
main_df=crsp[['year', 'ticker', 'siccd' ]].drop_duplicates()
grouped_df = news.groupby(['year', 'ticker']).size().reset_index(name='count')
merged_df = main_df.merge(grouped_df, on=['year', 'ticker'], how='left')
main=merged_df
main.fillna(0)

agg_df = main.groupby(['ticker', 'siccd'])['count'].sum().reset_index()
agg_df['coverage_dummy'] = np.where(agg_df['count'] > 0, 1, 0)
agg_df

fig2= pd.DataFrame(columns=['Code', 'Covered', 'Not Covered'])
agg_df['siccd'] = pd.to_numeric(agg_df['siccd'], errors='coerce').fillna(-1).astype(int)

codes=range(0,10)

for code in codes:
  fig2.loc[len(fig2)]=[code, len(agg_df[(agg_df['siccd'] == code) & (agg_df['coverage_dummy'] == 1)]) / len(agg_df) * 100, len(agg_df[(agg_df['siccd'] == code) & (agg_df['coverage_dummy'] == 0)]) / len(agg_df) * 100]

import matplotlib.pyplot as plt

# Setting up the bar width and positions for each bar
barWidth = 0.35
r1 = range(len(fig2))
r2 = [x + barWidth for x in r1]

# Creating the bars
plt.bar(r1, fig2['Covered'], width=barWidth, color='grey', edgecolor='grey', label='Covered')
plt.bar(r2, fig2['Not Covered'], width=barWidth, color='black', edgecolor='grey', label='Not Covered')

# Title & Subtitle
plt.title('Percentage of sample by 1-digit SIC code and Coverage')
plt.xlabel('1-digit SIC code', fontweight='bold')
plt.ylabel('Percent of sample', fontweight='bold')

# X axis
plt.xticks([r + barWidth for r in range(len(fig2))], fig2['Code'].astype(int))

# Legend & Show
plt.legend()
plt.show()

main.fillna(0)
years =main.groupby('year')['count'].sum()
years

main

"""# Table 2"""

main_df=crsp[['year', 'ticker', 'permno']].drop_duplicates()
main_df
# Step 1: Make a list of unique tickers in crsp where altprc is less than 5
low_priced_tickers = crsp[crsp['altprc'] < 5]['ticker'].unique()

# Step 2: Filter out these tickers from main_df
main_df = main_df[~main_df['ticker'].isin(low_priced_tickers)]

""" Add in average market capitalisation"""

avg_mkt_cap = crsp.groupby(['ticker', 'year'])['mkt_cap'].mean().reset_index()
main_df= main_df.merge(avg_mkt_cap, on=['ticker', 'year'], how='left')
main_df

"""Add in B/M Ratio"""

compustat=pd.read_csv(filepath + "compustat.csv")
compustat= compustat.rename(columns=str.lower)
x=['gvkey', 'cusip', 'lpermno', 'datadate', 'tic',  'seq', 'ceq',
                    'at', 'lt', 'txditc', 'txdb', 'itcb', 'pstkrv', 'pstkl', 'pstk', 'indfmt', 'datafmt',
                     'revt', 'cogs', 'xint', 'xsga', 'xrd', 'rect', 'invt', 'xpp', 'drc', 'drlt', 'ap', 'xacc',
                      'cstk', 'caps', 'tstk', 're', 'acominc', 'mkvalt']

compustat=compustat[x]

dates=['datadate']
for date in dates:
  compustat[date]=pd.to_datetime(compustat[date], format="%Y%m%d", errors='coerce')
compustat=compustat[compustat['datadate']>"2016-01-01"]

#filter columns
compustat['datafmt']=compustat['datafmt'].apply(str)
#compustat=compustat[compustat["indfmt"]=="INDL"]
compustat=compustat[compustat['datafmt']=="STD"]

#Only keep valid links
#compustat=compustat[(compustat["linktype"]=="LU")| (compustat["linktype"]=="LC")]

#Only keep links active at datadate
#compustat=compustat[(compustat['datadate']<=compustat["linkenddt"]) | pd.isnull(compustat['linkenddt'])]

compustat.to_sql("compustat", conn, if_exists='replace')

query='''
        SELECT DISTINCT *,
        COALESCE(seq, ceq+pstk, at-lt) + COALESCE(txditc, txdb + itcb, 0)- COALESCE(pstkrv, pstkl, pstk, 0) as book_value
        FROM compustat'''

compustat_calc=pd.read_sql(query, conn)
compustat=compustat_calc

compustat['datadate']=pd.to_datetime(compustat['datadate'])
compustat['quarter']=compustat['datadate'].dt.quarter
compustat=compustat[compustat['quarter']==4]

compustat['reference_year']=compustat['datadate'].dt.year +1

#Merge book_value into CRSP data
crsp['reference_year']=crsp['date'].dt.year
compustat.rename(columns={'lpermno':'permno'}, inplace=True)
merged_df = crsp.merge(compustat[['reference_year', 'permno', 'book_value']],
                       on=['reference_year', 'permno'],
                       how='left')

crsp=merged_df

crsp['month'] = pd.to_datetime(crsp['date']).dt.month
crsp['year']=pd.to_datetime(crsp['date']).dt.year
crsp_december = crsp[crsp['month'] == 12]

crsp_december['reference_year_i']=crsp_december['year']+1

crsp_december.rename(columns={'mkt_cap':'mkt_cap_bm'}, inplace=True)

# Rename year column in crsp_december to reference_year
crsp_december = crsp_december.rename(columns={'year': 'reference_year'})

# Perform the merge
# Merge crsp (left DataFrame) with crsp_december
crsp = crsp.merge(crsp_december[['ticker', 'reference_year_i', 'mkt_cap_bm']],
                         left_on=['ticker', 'year'],
                         right_on=['ticker', 'reference_year_i'],
                         how='left')

crsp['b/m']=crsp['book_value']/crsp['mkt_cap_bm']
crsp_bm=crsp[['ticker', 'year', 'b/m']].drop_duplicates()

main_df= main_df.merge(crsp_bm, on=['ticker', 'year'], how='left')
main_df

main_df = main_df[~main_df['year'].isin([2022, 2016])]
main_df

"""Add in Idiosyncratic Volatility"""

main="/content/gdrive/MyDrive/"

iv=pd.read_csv(main + "idiosyncratic_volatility.csv")
iv

main_df = main_df.merge(iv, on=['year', 'ticker'], how='left')
main_df

"""**IBES DATALOAD**"""

ibes=pd.read_csv(main + "clean_ibes_analyst_earnings.csv")

ibes['FPEDATS']=pd.to_datetime(ibes['FPEDATS'])
ibes['year']=ibes['FPEDATS'].dt.year
ibes=ibes[['year', 'TICKER', 'NUMEST', 'MEANEST', 'STDEV']]
ibes

main_df

ibes.rename(columns={'TICKER':'ticker'}, inplace=True)

main_df = main_df.merge(ibes, on=['year', 'ticker'], how='left')
main_df

"""**Add in Past Year Return**"""

crsp['ret_m']=crsp['ret']/100 +1

past_return= crsp.groupby(['year', 'ticker'])['ret_m'].prod().reset_index()

main_df= main_df.merge(past_return, on=['year', 'ticker'], how='left')
main_df

main_df.dropna(inplace=True)

file_path = "/content/gdrive/MyDrive/"
news=pd.read_csv(file_path + 'clean-articles.csv')
news['publishedDate'] = pd.to_datetime(news['publishedDate'])

# Extract year and create a new column
news['year'] = news['publishedDate'].dt.year
news['month'] = news['publishedDate'].dt.month

news_grouped = news.groupby(['ticker', 'year'])['articles'].count().reset_index()
news_grouped

main_df = main_df.merge(news_grouped, on=['year', 'ticker'], how='left')
main_df.fillna(0)

import numpy as np
main_df = main_df[main_df['b/m'] >0]

main_df['analyst_coverage'] = np.log(1 + main_df['NUMEST'])

main_df['analyst_dispersion'] = np.log(1 + (main_df['STDEV'] / abs(main_df['MEANEST'])))

main_df['book_to_market']=np.log(main_df['b/m'])

main_df['idiosyncratic_volatility']=np.log(main_df['residual_std_dev'])

main_df['past_year_return']=abs((main_df['ret_m']-1)*100)

main_df['size']=np.log(main_df['mkt_cap'])

# Fill NaN values in 'articles' with 0
main_df['articles'].fillna(0, inplace=True)

# Replace infinite values with NaN
main_df = main_df.replace([np.inf, -np.inf], np.nan)

# Drop rows with NaN values
main_df = main_df.dropna()
main_df

main_df['year'].value_counts()

import statsmodels.api as sm
import statsmodels.formula.api as smf

# Fill NaN values in 'articles' with 0
main_df['articles'].fillna(0, inplace=True)

# Define regression formula
formula = "articles ~ analyst_coverage + analyst_dispersion + book_to_market + idiosyncratic_volatility + past_year_return + size"

# Run OLS regression
model = smf.ols(formula=formula, data=main_df).fit(cov_type='HAC', cov_kwds={'maxlags': 1})

# Print summary with Newey-West standard errors
print(model.summary())

import pandas as pd
import statsmodels.formula.api as smf

# Extract unique years
unique_years = main_df['year'].unique()

# Placeholder for coefficients
coef_list = []

# Loop over years
for year in unique_years:
    temp_df = main_df[main_df['year'] == year]

    # Check if there's enough data in the current year to run the regression
    if len(temp_df) > len(temp_df.columns):
        # Run the regression
        model = smf.ols(formula=formula, data=temp_df).fit(cov_type='HAC', cov_kwds={'maxlags': 1})

        # Append coefficients to the list
        coef_list.append(model.params)

# Convert list of coefficients to DataFrame
coef_df = pd.DataFrame(coef_list, index=unique_years)

# Take the mean of the coefficients
mean_coef = coef_df.mean()

print(mean_coef)

negative_bm_count = (main_df['b/m'] < 0).sum()
print(f"Number of negative b/m values: {negative_bm_count}")

"""# Table 4"""

from scipy.stats.mstats import winsorize

crsp['_ret'] = winsorize(crsp['ret'], limits=(0, 0.01))
crsp['next_month_ret'] = crsp.groupby('ticker')['ret'].shift(-1)
crsp['prior_month_ret'] = crsp.groupby('ticker')['ret'].shift(1)

file_path = "/content/gdrive/MyDrive/"
news=pd.read_csv(file_path + 'clean-articles.csv')
news

news['publishedDate'] = pd.to_datetime(news['publishedDate'])

# Extract year and create a new column
news['year'] = news['publishedDate'].dt.year
news['month'] = news['publishedDate'].dt.month

news_grouped = news.groupby(['ticker', 'year', 'month'])['articles'].count().reset_index()
news_grouped

crsp['month']=crsp['date'].dt.month

crsp = crsp.merge(news_grouped, on=['ticker', 'year', 'month'], how='left')
crsp['articles'].fillna(0, inplace=True)

crsp['coverage_dummy'] = np.where(crsp['articles'] > 0, 1, 0)

median_articles = crsp[crsp['coverage_dummy'] == 1].groupby(['year', 'month'])['articles'].median().reset_index()
median_articles.rename(columns={'articles':'median_articles'}, inplace=True)
crsp = pd.merge(crsp, median_articles, on=['year', 'month'], how='left')

import numpy as np

conditions = [
    crsp['articles'] == 0,
    crsp['articles'] < crsp['median_articles'],
    crsp['articles'] > crsp['median_articles']
]

choices = ['no', 'low', 'high']

crsp['media_portfolio'] = np.select(conditions, choices, default='no')
crsp['media_portfolio'].value_counts()

"""**Returns for all stocks**"""

crsp['ret']

portfolio_returns = crsp.groupby(['year', 'month', 'media_portfolio']).agg(
    mean_return=('next_month_ret', 'mean'),
    count_obs=('next_month_ret', 'size')
).reset_index()
portfolio_returns

# Group by and aggregate
portfolio_stats = crsp.groupby(['year', 'month', 'media_portfolio']).agg(
    mean_return=('next_month_ret', 'mean'),
    count_obs=('next_month_ret', 'size')
).reset_index()

# Pivot the table
portfolio_pivot = portfolio_stats.pivot_table(index=['year', 'month'],
                                              columns='media_portfolio',
                                              values=['mean_return', 'count_obs'])

# Reset index to flatten rows
portfolio_pivot.reset_index(inplace=True)

# Flatten column multi-index
portfolio_pivot.columns = ['_'.join(map(str, col)).rstrip('_') for col in portfolio_pivot.columns.values]
portfolio_pivot

portfolio_pivot['no-high']=portfolio_pivot['mean_return_no']-portfolio_pivot['mean_return_high']
portfolio_pivot.mean()

import numpy as np

# Extract the data
data = portfolio_pivot['no-high'].dropna()  # Drop NaN values

# Calculate t-statistic
sample_mean = data.mean()
sample_std = data.std()
sample_size = len(data)
t_stat = sample_mean / (sample_std / np.sqrt(sample_size))

print("t-statistic:", t_stat)

"""**By Market Size**"""

# Define a function to assign portfolios
def assign_portfolio(mkt_cap_series):
    # Create the quantile boundaries
    boundaries = [
        mkt_cap_series.quantile(0.33),
        mkt_cap_series.quantile(0.66),
        mkt_cap_series.max()
    ]

    # Assign portfolios based on the boundaries
    return pd.cut(
        mkt_cap_series,
        bins=[mkt_cap_series.min()-0.01] + boundaries,
        labels=['1', '2', '3']
    )

# Group by year and month, then apply the function
crsp['size_portfolio'] = crsp.groupby(['year', 'month'])['mkt_cap'].transform(assign_portfolio)

# Group by and aggregate
portfolio_stats = crsp.groupby(['year', 'month', 'media_portfolio', 'size_portfolio']).agg(
    mean_return=('next_month_ret', 'mean'),
    count_obs=('next_month_ret', 'size')
).reset_index()

# Pivot the table
portfolio_pivot = portfolio_stats.pivot_table(index=['year', 'month', 'size_portfolio'],
                                              columns='media_portfolio',
                                              values=['mean_return', 'count_obs'])

# Reset index to flatten rows
portfolio_pivot.reset_index(inplace=True)

# Flatten column multi-index
portfolio_pivot.columns = ['_'.join(map(str, col)).rstrip('_') for col in portfolio_pivot.columns.values]
portfolio_pivot['no-high']=portfolio_pivot['mean_return_no']-portfolio_pivot['mean_return_high']
portfolio_pivot

size_portfolio_means = portfolio_pivot.groupby('size_portfolio').agg({
    'count_obs_high': 'mean',
    'count_obs_low': 'mean',
    'count_obs_no': 'mean',
    'mean_return_high': 'mean',
    'mean_return_low': 'mean',
    'mean_return_no': 'mean',
    'no-high': 'mean'
}).reset_index()
size_portfolio_means

import numpy as np

# Aggregate functions for mean, standard deviation and count
aggregations = {
    'no-high': ['mean', 'std', 'size']
}

size_portfolio_stats = portfolio_pivot.groupby('size_portfolio').agg(aggregations).reset_index()

# Compute standard error
size_portfolio_stats['no-high', 'SE'] = size_portfolio_stats['no-high', 'std'] / np.sqrt(size_portfolio_stats['no-high', 'size'])

# Compute t-statistic for mean of no-high column
size_portfolio_stats['no-high', 't-stat'] = size_portfolio_stats['no-high', 'mean'] / size_portfolio_stats['no-high', 'SE']
size_portfolio_stats

"""**By Past Month Return**"""

# Define a function to assign portfolios
def assign_portfolio(mkt_cap_series):
    # Create the quantile boundaries
    boundaries = [
        mkt_cap_series.quantile(0.33),
        mkt_cap_series.quantile(0.66),
        mkt_cap_series.max()
    ]

    # Assign portfolios based on the boundaries
    return pd.cut(
        mkt_cap_series,
        bins=[mkt_cap_series.min()-0.01] + boundaries,
        labels=['1', '2', '3']
    )

# Group by year and month, then apply the function
crsp['prior_portfolio'] = crsp.groupby(['year', 'month'])['prior_month_ret'].transform(assign_portfolio)

# Group by and aggregate
portfolio_stats = crsp.groupby(['year', 'month', 'media_portfolio', 'prior_portfolio']).agg(
    mean_return=('next_month_ret', 'mean'),
    count_obs=('next_month_ret', 'size')
).reset_index()

# Pivot the table
portfolio_pivot = portfolio_stats.pivot_table(index=['year', 'month', 'prior_portfolio'],
                                              columns='media_portfolio',
                                              values=['mean_return', 'count_obs'])

# Reset index to flatten rows
portfolio_pivot.reset_index(inplace=True)

# Flatten column multi-index
portfolio_pivot.columns = ['_'.join(map(str, col)).rstrip('_') for col in portfolio_pivot.columns.values]
portfolio_pivot['no-high']=portfolio_pivot['mean_return_no']-portfolio_pivot['mean_return_high']
portfolio_pivot

prior_portfolio_means = portfolio_pivot.groupby('prior_portfolio').agg({
    'count_obs_high': 'mean',
    'count_obs_low': 'mean',
    'count_obs_no': 'mean',
    'mean_return_high': 'mean',
    'mean_return_low': 'mean',
    'mean_return_no': 'mean',
    'no-high': 'mean'
}).reset_index()
prior_portfolio_means

import numpy as np

# Aggregate functions for mean, standard deviation and count
aggregations = {
    'no-high': ['mean', 'std', 'size']
}

size_portfolio_stats = portfolio_pivot.groupby('prior_portfolio').agg(aggregations).reset_index()

# Compute standard error
size_portfolio_stats['no-high', 'SE'] = size_portfolio_stats['no-high', 'std'] / np.sqrt(size_portfolio_stats['no-high', 'size'])

# Compute t-statistic for mean of no-high column
size_portfolio_stats['no-high', 't-stat'] = size_portfolio_stats['no-high', 'mean'] / size_portfolio_stats['no-high', 'SE']
size_portfolio_stats

"""**By Current Month Return**"""

# Define a function to assign portfolios
def assign_portfolio(mkt_cap_series):
    # Create the quantile boundaries
    boundaries = [
        mkt_cap_series.quantile(0.33),
        mkt_cap_series.quantile(0.66),
        mkt_cap_series.max()
    ]

    # Assign portfolios based on the boundaries
    return pd.cut(
        mkt_cap_series,
        bins=[mkt_cap_series.min()-0.01] + boundaries,
        labels=['1', '2', '3']
    )

# Group by year and month, then apply the function
crsp['current_portfolio'] = crsp.groupby(['year', 'month'])['ret'].transform(assign_portfolio)

# Group by and aggregate
portfolio_stats = crsp.groupby(['year', 'month', 'media_portfolio', 'current_portfolio']).agg(
    mean_return=('next_month_ret', 'mean'),
    count_obs=('next_month_ret', 'size')
).reset_index()

# Pivot the table
portfolio_pivot = portfolio_stats.pivot_table(index=['year', 'month', 'current_portfolio'],
                                              columns='media_portfolio',
                                              values=['mean_return', 'count_obs'])

# Reset index to flatten rows
portfolio_pivot.reset_index(inplace=True)

# Flatten column multi-index
portfolio_pivot.columns = ['_'.join(map(str, col)).rstrip('_') for col in portfolio_pivot.columns.values]
portfolio_pivot['no-high']=portfolio_pivot['mean_return_no']-portfolio_pivot['mean_return_high']
portfolio_pivot

prior_portfolio_means = portfolio_pivot.groupby('current_portfolio').agg({
    'count_obs_high': 'mean',
    'count_obs_low': 'mean',
    'count_obs_no': 'mean',
    'mean_return_high': 'mean',
    'mean_return_low': 'mean',
    'mean_return_no': 'mean',
    'no-high': 'mean'
}).reset_index()
prior_portfolio_means

import numpy as np

# Aggregate functions for mean, standard deviation and count
aggregations = {
    'no-high': ['mean', 'std', 'size']
}

size_portfolio_stats = portfolio_pivot.groupby('current_portfolio').agg(aggregations).reset_index()

# Compute standard error
size_portfolio_stats['no-high', 'SE'] = size_portfolio_stats['no-high', 'std'] / np.sqrt(size_portfolio_stats['no-high', 'size'])

# Compute t-statistic for mean of no-high column
size_portfolio_stats['no-high', 't-stat'] = size_portfolio_stats['no-high', 'mean'] / size_portfolio_stats['no-high', 'SE']
size_portfolio_stats

"""**By Price**"""

# Define a function to assign portfolios
def assign_portfolio(mkt_cap_series):
    # Create the quantile boundaries
    boundaries = [
        mkt_cap_series.quantile(0.33),
        mkt_cap_series.quantile(0.66),
        mkt_cap_series.max()
    ]

    # Assign portfolios based on the boundaries
    return pd.cut(
        mkt_cap_series,
        bins=[mkt_cap_series.min()-0.01] + boundaries,
        labels=['1', '2', '3']
    )

# Group by year and month, then apply the function
crsp['price_portfolio'] = crsp.groupby(['year', 'month'])['altprc'].transform(assign_portfolio)

# Group by and aggregate
portfolio_stats = crsp.groupby(['year', 'month', 'media_portfolio', 'price_portfolio']).agg(
    mean_return=('next_month_ret', 'mean'),
    count_obs=('next_month_ret', 'size')
).reset_index()

# Pivot the table
portfolio_pivot = portfolio_stats.pivot_table(index=['year', 'month', 'price_portfolio'],
                                              columns='media_portfolio',
                                              values=['mean_return', 'count_obs'])

# Reset index to flatten rows
portfolio_pivot.reset_index(inplace=True)

# Flatten column multi-index
portfolio_pivot.columns = ['_'.join(map(str, col)).rstrip('_') for col in portfolio_pivot.columns.values]
portfolio_pivot['no-high']=portfolio_pivot['mean_return_no']-portfolio_pivot['mean_return_high']
portfolio_pivot

prior_portfolio_means = portfolio_pivot.groupby('price_portfolio').agg({
    'count_obs_high': 'mean',
    'count_obs_low': 'mean',
    'count_obs_no': 'mean',
    'mean_return_high': 'mean',
    'mean_return_low': 'mean',
    'mean_return_no': 'mean',
    'no-high': 'mean'
}).reset_index()
prior_portfolio_means

import numpy as np

# Aggregate functions for mean, standard deviation and count
aggregations = {
    'no-high': ['mean', 'std', 'size']
}

size_portfolio_stats = portfolio_pivot.groupby('price_portfolio').agg(aggregations).reset_index()

# Compute standard error
size_portfolio_stats['no-high', 'SE'] = size_portfolio_stats['no-high', 'std'] / np.sqrt(size_portfolio_stats['no-high', 'size'])

# Compute t-statistic for mean of no-high column
size_portfolio_stats['no-high', 't-stat'] = size_portfolio_stats['no-high', 'mean'] / size_portfolio_stats['no-high', 'SE']
size_portfolio_stats

"""# Table 5"""

crsp['next_month_ret'] = crsp.groupby('ticker')['ret'].shift(-1)
file_path = "/content/gdrive/MyDrive/"
news=pd.read_csv(file_path + 'clean-articles.csv')
news['publishedDate'] = pd.to_datetime(news['publishedDate'])

# Extract year and create a new column
news['year'] = news['publishedDate'].dt.year
news['month'] = news['publishedDate'].dt.month

news_grouped = news.groupby(['ticker', 'year', 'month'])['articles'].count().reset_index()
news_grouped

crsp['month']=crsp['date'].dt.month
crsp = crsp.merge(news_grouped, on=['ticker', 'year', 'month'], how='left')
crsp['articles'].fillna(0, inplace=True)
crsp['coverage_dummy'] = np.where(crsp['articles'] > 0, 1, 0)


median_articles = crsp[crsp['coverage_dummy'] == 1].groupby(['year', 'month'])['articles'].median().reset_index()
median_articles.rename(columns={'articles':'median_articles'}, inplace=True)
crsp = pd.merge(crsp, median_articles, on=['year', 'month'], how='left')

import numpy as np

conditions = [
    crsp['articles'] == 0,
    crsp['articles'] < crsp['median_articles'],
    crsp['articles'] > crsp['median_articles']
]

choices = ['no', 'low', 'high']

crsp['media_portfolio'] = np.select(conditions, choices, default='no')
crsp['media_portfolio'].value_counts()

portfolio_returns = crsp.groupby(['year', 'month', 'media_portfolio']).agg(
    mean_return=('next_month_ret', 'mean'),
    count_obs=('next_month_ret', 'size')
).reset_index()
# Group by and aggregate
portfolio_stats = crsp.groupby(['year', 'month', 'media_portfolio']).agg(
    mean_return=('next_month_ret', 'mean')
).reset_index()

# Pivot the table
portfolio_pivot = portfolio_stats.pivot_table(index=['year', 'month'],
                                              columns='media_portfolio',
                                              values=['mean_return'])

# Reset index to flatten rows
portfolio_pivot.reset_index(inplace=True)

# Flatten column multi-index
portfolio_pivot.columns = ['_'.join(map(str, col)).rstrip('_') for col in portfolio_pivot.columns.values]
portfolio_pivot

main=portfolio_pivot

"""Import Fama French Factors"""

import pandas_datareader.data as web
from pandas_datareader.famafrench import get_available_datasets
datasets = get_available_datasets()
#FF 5 FACTORS + MOMENTUM FACTOR
df_5_factor=[dataset for dataset in datasets if 'Research' in dataset and 'Factor' in dataset]
df_mom_factor=[dataset for dataset in datasets if 'Momentum' in dataset and 'Factor' in dataset]

ff=web.DataReader(df_5_factor[3],'famafrench',start='1963-07-01',end='2022-11-01')[0]
ff_mom=web.DataReader(df_mom_factor[0],'famafrench',start='1963-07-01',end='2022-11-01')[0]

ff=pd.merge(ff, ff_mom, on='Date')
ff.reset_index(inplace=True)
ff['Date']=ff['Date'].apply(str)
ff['Date']=pd.to_datetime(ff['Date'])
ff['month']=ff['Date'].dt.month
ff['year']=ff['Date'].dt.year

"""Import Pastor and Stambaugh (2003) Traded Liquidity Factor (PSLIQ)"""

psliq=pd.read_csv(filepath + "PSLIQ.csv")
psliq['DATE'] = pd.to_datetime(psliq['DATE'], format="%Y%m%d")
psliq.rename(columns={'DATE':'Date'}, inplace=True)
psliq['month']=psliq['Date'].dt.month
psliq['year']=psliq['Date'].dt.year

psliq.drop(['Date', 'PS_INNOV', 'PS_LEVEL'], axis=1, inplace=True)

# First, merge main with ff
merged_with_ff = main.merge(ff, on=['year', 'month'], how='left')

# Then, merge the result with psliq
final_merged = merged_with_ff.merge(psliq, on=['year', 'month'], how='left')
main=final_merged
main

"""CAPM Regression"""

main['no-high']=main['mean_return_no']-main['mean_return_high']

main

import statsmodels.api as sm

X = sm.add_constant(main['Mkt-RF'])
y = main['mean_return_high']
main.dropna(inplace=True)
# Perform regression
model = sm.OLS(y, X).fit()

# Print summary
print(model.summary())

X = sm.add_constant(main[['Mkt-RF', 'SMB', 'HML']])
y = main['mean_return_high']
# Perform the regression
model = sm.OLS(y, X).fit()

# Display the results
print(model.summary())

X = sm.add_constant(main[['Mkt-RF', 'SMB', 'HML', 'Mom   ']])
y = main['mean_return_high']

# Perform the regression
model = sm.OLS(y, X).fit()

# Display the results
print(model.summary())

X = sm.add_constant(main[['Mkt-RF', 'SMB', 'HML', 'Mom   ', 'PS_VWF']])
y = main['mean_return_no']

# Perform the regression
model = sm.OLS(y, X).fit()

# Display the results
print(model.summary())